{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329a9dc3",
   "metadata": {},
   "source": [
    "### False Positive Test Set Evaluation\n",
    "[Go to BERT](#1)      \n",
    "[Go to RoBERTa](#2)       \n",
    "[Go to Mental-RoBERTa](#3)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16a8c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Documents\\Master_Project\\Suicide-Ideation-Detection-in-Social-Media-Using-Personality-Traits-main\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, BertForSequenceClassification, BertTokenizer, AutoModel, AutoTokenizer, BertModel, AutoConfig, RobertaPreTrainedModel, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# ===== Load and clean dataset =====\n",
    "df = pd.read_csv('../dataset/false_positive_test_set_with_personality.csv')\n",
    "\n",
    "df['Post'] = df['Post'].str.strip(\"[]'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7961c2",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "652762c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[15 74]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.8315\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.17      0.29        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.17        89\n",
      "   macro avg       0.50      0.08      0.14        89\n",
      "weighted avg       1.00      0.17      0.29        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# BERT with raw 2000 samples\n",
    "\n",
    "model_dir = \"../saved_models/bert_raw\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dccf636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[14 75]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.8427\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.16      0.27        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.16        89\n",
      "   macro avg       0.50      0.08      0.14        89\n",
      "weighted avg       1.00      0.16      0.27        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# BERT with original labels 2000 samples + personality (benchmark)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertPreTrainedModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# ===== Load and clean dataset =====\n",
    "df = pd.read_csv('../dataset/false_positive_test_set_with_personality.csv')\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/bert_benchmark\"\n",
    "\n",
    "# Custom model combining BERT + personality features\n",
    "class BertWithPersonality(BertPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = BertWithPersonality.from_pretrained(model_dir, config=config)\n",
    "#model.load_state_dict(torch.load(f\"{model_dir}/pytorch_model.bin\")) \n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask, personality_feats=personality_feats)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f828a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[57 32]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.3596\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.64      0.78        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.64        89\n",
      "   macro avg       0.50      0.32      0.39        89\n",
      "weighted avg       1.00      0.64      0.78        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# BERT with relabeled 2000 samples\n",
    "\n",
    "model_dir = \"../saved_models/bert_relabeled\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "644513d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[48 41]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.4607\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.54      0.70        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.54        89\n",
      "   macro avg       0.50      0.27      0.35        89\n",
      "weighted avg       1.00      0.54      0.70        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# BERT with relabeled 2000 samples + personality\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertPreTrainedModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/bert_personality\"\n",
    "\n",
    "# Custom model combining BERT + personality features\n",
    "class BertWithPersonality(BertPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "model = BertWithPersonality.from_pretrained(model_dir, config=config)\n",
    "model.load_state_dict(torch.load(f\"{model_dir}/pytorch_model.bin\")) \n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask, personality_feats=personality_feats)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab4ecb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[65 24]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.2697\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.73      0.84        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.73        89\n",
      "   macro avg       0.50      0.37      0.42        89\n",
      "weighted avg       1.00      0.73      0.84        89\n",
      "\n",
      "Evaluation completed and results saved with NLI filtering.\n"
     ]
    }
   ],
   "source": [
    "# BERT with NLI\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ===== Load BERT suicide classifier =====\n",
    "model_dir = \"../saved_models/bert_relabeled\"\n",
    "bert_model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device).eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Helper: NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Run inference with BERT classifier =====\n",
    "encodings = bert_tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== Apply NLI filtering for BERT suicide predictions =====\n",
    "final_predictions = []\n",
    "for text, pred in zip(df['Post'], predictions):\n",
    "    if pred == 1:  # initially predicted as suicide\n",
    "        is_negative = nli_negative_filter(text, threshold=0.62)\n",
    "        final_predictions.append(0 if is_negative else 1)\n",
    "    else:\n",
    "        final_predictions.append(0)\n",
    "\n",
    "# ===== Evaluation =====\n",
    "true_labels = [0] * len(df)  # all ground truth non-suicide\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, final_predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir + '/false_positive_test_with_nli.csv', index=False)\n",
    "print(\"Evaluation completed and results saved with NLI filtering.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf2cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertWithPersonality were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix after NLI filtering:\n",
      "[[58 31]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.3483\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.65      0.79        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.65        89\n",
      "   macro avg       0.50      0.33      0.39        89\n",
      "weighted avg       1.00      0.65      0.79        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# BERT with relabeled 2000 samples + personality + NLI\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, AutoTokenizer, AutoModelForSequenceClassification, BertPreTrainedModel, BertConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/bert_personality\"\n",
    "\n",
    "# Custom model combining BERT + personality features\n",
    "class BertWithPersonality(BertPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = BertWithPersonality.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "model.load_state_dict(torch.load(f\"{model_dir}/pytorch_model.bin\")) \n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Helper: NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference with NLI filtering =====\n",
    "filtered_predictions = []\n",
    "\n",
    "for i, post in enumerate(df['Post'].tolist()):\n",
    "    # BERT + personality prediction\n",
    "    with torch.no_grad():\n",
    "        bert_logits = model(\n",
    "            input_ids[i].unsqueeze(0),\n",
    "            attention_mask=attention_mask[i].unsqueeze(0),\n",
    "            personality_feats=personality_feats[i].unsqueeze(0)\n",
    "        )\n",
    "        bert_pred = torch.argmax(bert_logits, dim=1).item()\n",
    "\n",
    "    if bert_pred == 1:  # initially predicted as suicide\n",
    "        is_negative = nli_negative_filter(post, threshold=0.62)\n",
    "        filtered_predictions.append(0 if is_negative else 1)\n",
    "    else:\n",
    "        filtered_predictions.append(0)\n",
    "\n",
    "\n",
    "# ===== Metrics =====\n",
    "true_labels = [0] * len(df)  # all non-suicidal in this FP test set\n",
    "\n",
    "cm = confusion_matrix(true_labels, filtered_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix after NLI filtering:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, filtered_predictions,\n",
    "                            target_names=[\"Non-suicide\", \"Suicide\"],\n",
    "                            zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df['Final_Predicted_Label'] = [\"suicide\" if p == 1 else \"non-suicide\" for p in filtered_predictions]\n",
    "df.to_csv(model_dir + '/false_positive_test_results_with_nli.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70deb6",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc8abea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10 79]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.8876\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.11      0.20        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.11        89\n",
      "   macro avg       0.50      0.06      0.10        89\n",
      "weighted avg       1.00      0.11      0.20        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# roBERTa with raw 2000 samples\n",
    "\n",
    "model_dir = \"../saved_models/roberta_raw\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba8974b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 6 83]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.9326\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.07      0.13        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.07        89\n",
      "   macro avg       0.50      0.03      0.06        89\n",
      "weighted avg       1.00      0.07      0.13        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa with original labels 2000 samples + personality (benchmark)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# ===== Load and clean dataset =====\n",
    "df = pd.read_csv('../dataset/false_positive_test_set_with_personality.csv')\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/roberta_raw_personality\"\n",
    "\n",
    "# Custom model combining BERT + personality features\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_dir,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask, personality_feats=personality_feats)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a1fad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[41 48]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.5393\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.46      0.63        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.46        89\n",
      "   macro avg       0.50      0.23      0.32        89\n",
      "weighted avg       1.00      0.46      0.63        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# roBERTa with relabeled 2000 samples\n",
    "\n",
    "model_dir = \"../saved_models/roberta_relabeled\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9beb8a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../saved_models/roberta_personality and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[73 16]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.1798\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.82      0.90        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.82        89\n",
      "   macro avg       0.50      0.41      0.45        89\n",
      "weighted avg       1.00      0.82      0.90        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa with relabeled 2000 samples + personality\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizerFast, RobertaModel, RobertaConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/roberta_personality\"\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = RobertaConfig.from_pretrained(model_dir, num_labels=2)\n",
    "model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "model.to(device).eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask, personality_feats=personality_feats)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ab55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[52 37]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.4157\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.58      0.74        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.58        89\n",
      "   macro avg       0.50      0.29      0.37        89\n",
      "weighted avg       1.00      0.58      0.74        89\n",
      "\n",
      "Evaluation completed and results saved with NLI filtering.\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa with NLI\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ===== Load BERT suicide classifier =====\n",
    "model_dir = \"../saved_models/roberta_relabeled\"\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device).eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Helper: NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Run inference with BERT classifier =====\n",
    "encodings = bert_tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== Apply NLI filtering for BERT suicide predictions =====\n",
    "final_predictions = []\n",
    "for text, pred in zip(df['Post'], predictions):\n",
    "    if pred == 1:  # initially predicted as suicide\n",
    "        is_negative = nli_negative_filter(text, threshold=0.65)\n",
    "        final_predictions.append(0 if is_negative else 1)\n",
    "    else:\n",
    "        final_predictions.append(0)\n",
    "\n",
    "# ===== Evaluation =====\n",
    "true_labels = [0] * len(df)  # all ground truth non-suicide\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, final_predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir + '/false_positive_test_with_nli.csv', index=False)\n",
    "print(\"Evaluation completed and results saved with NLI filtering.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9fd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../saved_models/roberta_personality and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix after NLI filtering:\n",
      "[[76 13]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.1461\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.85      0.92        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.85        89\n",
      "   macro avg       0.50      0.43      0.46        89\n",
      "weighted avg       1.00      0.85      0.92        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa with relabeled 2000 samples + personality + NLI\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/roberta_personality\"\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = AutoModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(model_dir, num_labels=2)\n",
    "model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "model.to(device).eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Helper: NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference with NLI filtering =====\n",
    "filtered_predictions = []\n",
    "\n",
    "for i, post in enumerate(df['Post'].tolist()):\n",
    "    # BERT + personality prediction\n",
    "    with torch.no_grad():\n",
    "        bert_logits = model(\n",
    "            input_ids[i].unsqueeze(0),\n",
    "            attention_mask=attention_mask[i].unsqueeze(0),\n",
    "            personality_feats=personality_feats[i].unsqueeze(0)\n",
    "        )\n",
    "        bert_pred = torch.argmax(bert_logits, dim=1).item()\n",
    "\n",
    "    if bert_pred == 1:  # initially predicted as suicide\n",
    "        is_negative = nli_negative_filter(post, threshold=0.65)\n",
    "        filtered_predictions.append(0 if is_negative else 1)\n",
    "    else:\n",
    "        filtered_predictions.append(0)\n",
    "\n",
    "\n",
    "# ===== Metrics =====\n",
    "true_labels = [0] * len(df)  # all non-suicidal in this FP test set\n",
    "\n",
    "cm = confusion_matrix(true_labels, filtered_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix after NLI filtering:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, filtered_predictions,\n",
    "                            target_names=[\"Non-suicide\", \"Suicide\"],\n",
    "                            zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df['Final_Predicted_Label'] = [\"suicide\" if p == 1 else \"non-suicide\" for p in filtered_predictions]\n",
    "df.to_csv(model_dir + '/false_positive_test_results_with_nli.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639cfde",
   "metadata": {},
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c98985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 6 83]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.9326\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.07      0.13        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.07        89\n",
      "   macro avg       0.50      0.03      0.06        89\n",
      "weighted avg       1.00      0.07      0.13        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# Mental-RoBERTa with raw 2000 samples\n",
    "\n",
    "model_dir = \"../saved_models/mental_roberta_raw\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cabb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../saved_models/mental_roberta_raw_personality and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 6 83]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.9326\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.07      0.13        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.07        89\n",
      "   macro avg       0.50      0.03      0.06        89\n",
      "weighted avg       1.00      0.07      0.13        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# Mental-RoBERTa with original labels 2000 samples + personality (benchmark)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# ===== Load and clean dataset =====\n",
    "df = pd.read_csv('../dataset/false_positive_test_set_with_personality.csv')\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/mental_roberta_raw_personality\"\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_dir,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask, personality_feats=personality_feats)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0044a124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[64 25]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.2809\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.72      0.84        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.72        89\n",
      "   macro avg       0.50      0.36      0.42        89\n",
      "weighted avg       1.00      0.72      0.84        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# Mental-RoBERTa with relabeled 2000 samples\n",
    "\n",
    "model_dir = \"../saved_models/mental_roberta_relabeled\"\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f17ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../saved_models/mental_roberta_personality and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[53 36]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.4045\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.60      0.75        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60        89\n",
      "   macro avg       0.50      0.30      0.37        89\n",
      "weighted avg       1.00      0.60      0.75        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# Mental-RoBERTa with relabeld 2000 samples + personality (benchmark)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# ===== Load and clean dataset =====\n",
    "df = pd.read_csv('../dataset/false_positive_test_set_with_personality.csv')\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/mental_roberta_personality\"\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_dir,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference =====\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask, personality_feats=personality_feats)\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== All ground-truth labels are 0 (non-suicide) =====\n",
    "true_labels = [0] * len(df)\n",
    "\n",
    "# ===== Add predictions to DataFrame =====\n",
    "# Map numeric labels to target names\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in predictions]\n",
    "\n",
    "# ===== Metrics =====\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "tn, fp, fn, tp = cm.ravel()  # Here fn, tp will be 0 because true_labels are all 0\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir+ '/false_positive_test_results.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d06d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[67 22]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.2472\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.75      0.86        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.75        89\n",
      "   macro avg       0.50      0.38      0.43        89\n",
      "weighted avg       1.00      0.75      0.86        89\n",
      "\n",
      "Evaluation completed and results saved with NLI filtering.\n"
     ]
    }
   ],
   "source": [
    "# Mental-RaBERTa with NLI\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# ===== Load Mental-RaBERTa suicide classifier =====\n",
    "model_dir = \"../saved_models/mental_roberta_relabeled\"\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device).eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Helper: NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Run inference with BERT classifier =====\n",
    "encodings = bert_tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# ===== Apply NLI filtering for BERT suicide predictions =====\n",
    "final_predictions = []\n",
    "for text, pred in zip(df['Post'], predictions):\n",
    "    if pred == 1:  # initially predicted as suicide\n",
    "        is_negative = nli_negative_filter(text, threshold=0.65)\n",
    "        final_predictions.append(0 if is_negative else 1)\n",
    "    else:\n",
    "        final_predictions.append(0)\n",
    "\n",
    "# ===== Evaluation =====\n",
    "true_labels = [0] * len(df)  # all ground truth non-suicide\n",
    "label_map = {0: \"non-suicide\", 1: \"suicide\"}\n",
    "df['Predicted_Label'] = [label_map[p] for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, final_predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df.to_csv(model_dir + '/false_positive_test_with_nli.csv', index=False)\n",
    "print(\"Evaluation completed and results saved with NLI filtering.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce2a17d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../saved_models/mental_roberta_personality and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix after NLI filtering:\n",
      "[[60 29]\n",
      " [ 0  0]]\n",
      "\n",
      "False Positive Rate (FPR): 0.3258\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       1.00      0.67      0.81        89\n",
      "     Suicide       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.67        89\n",
      "   macro avg       0.50      0.34      0.40        89\n",
      "weighted avg       1.00      0.67      0.81        89\n",
      "\n",
      "Evaluation completed and results saved.\n"
     ]
    }
   ],
   "source": [
    "# Mental-RoBERTa with relabeled 2000 samples + personality + NLI\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "personality_cols = [\"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
    "personality_feats = torch.tensor(df[personality_cols].values, dtype=torch.float)\n",
    "\n",
    "model_dir = \"../saved_models/mental_roberta_personality\"\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_dir,\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Helper: NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Tokenize =====\n",
    "encodings = tokenizer(\n",
    "    df['Post'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "personality_feats = personality_feats.to(device)\n",
    "\n",
    "# ===== Inference with NLI filtering =====\n",
    "filtered_predictions = []\n",
    "\n",
    "for i, post in enumerate(df['Post'].tolist()):\n",
    "    # BERT + personality prediction\n",
    "    with torch.no_grad():\n",
    "        bert_logits = model(\n",
    "            input_ids[i].unsqueeze(0),\n",
    "            attention_mask=attention_mask[i].unsqueeze(0),\n",
    "            personality_feats=personality_feats[i].unsqueeze(0)\n",
    "        )\n",
    "        bert_pred = torch.argmax(bert_logits, dim=1).item()\n",
    "\n",
    "    if bert_pred == 1:  # initially predicted as suicide\n",
    "        is_negative = nli_negative_filter(post, threshold=0.65)\n",
    "        filtered_predictions.append(0 if is_negative else 1)\n",
    "    else:\n",
    "        filtered_predictions.append(0)\n",
    "\n",
    "\n",
    "# ===== Metrics =====\n",
    "true_labels = [0] * len(df)  # all non-suicidal in this FP test set\n",
    "\n",
    "cm = confusion_matrix(true_labels, filtered_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix after NLI filtering:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, filtered_predictions,\n",
    "                            target_names=[\"Non-suicide\", \"Suicide\"],\n",
    "                            zero_division=0))\n",
    "\n",
    "# ===== Save results =====\n",
    "df['Final_Predicted_Label'] = [\"suicide\" if p == 1 else \"non-suicide\" for p in filtered_predictions]\n",
    "df.to_csv(model_dir + '/false_positive_test_results_with_nli.csv', index=False)\n",
    "print(\"Evaluation completed and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
