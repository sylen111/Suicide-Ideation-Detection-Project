{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7250c14",
   "metadata": {},
   "source": [
    "### RoBERTa\n",
    "[Training - RoBERTa + Original labels](#1)  \n",
    "[Training - RoBERTa + Original labels + Personality (Benchmark)](#2)            \n",
    "[Training - RoBERTa + Relabeled data](#3)       \n",
    "[Training - RoBERTa + Relabeled + Personality](#4)      \n",
    "\n",
    "<u>[Testing results](#5)</u>  \n",
    "[RoBERTa + Original labels](#6)  \n",
    "[RoBERTa + Original labels + Personality (Benchmark)](#7)            \n",
    "[RoBERTa + Relabeled data](#8)       \n",
    "[RoBERTa + Relabeled + Personality](#9)     \n",
    "[RoBERTa + Relabeled data + NLI](#10)       \n",
    "[RoBERTa + Relabeled + Personality + NLI](#11)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd83553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set the seed value\n",
    "seed_val = 42\n",
    "\n",
    "# Python random\n",
    "random.seed(seed_val)\n",
    "\n",
    "# NumPy random\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# PyTorch random\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bba445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, RobertaModel, RobertaConfig, RobertaPreTrainedModel, AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2037be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>text</th>\n",
       "      <th>ori_class</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>true_class</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>neuroticism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79033</td>\n",
       "      <td>Here are some jokes that don't have a punchlin...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>Here are some jokes that don't have a punchlin...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.862079</td>\n",
       "      <td>0.632528</td>\n",
       "      <td>0.299561</td>\n",
       "      <td>0.650520</td>\n",
       "      <td>0.004304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257115</td>\n",
       "      <td>I'll begin to blog on here before I end it on ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>I'll begin to blog on here before I end it on ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.561344</td>\n",
       "      <td>0.527004</td>\n",
       "      <td>0.222416</td>\n",
       "      <td>0.461801</td>\n",
       "      <td>0.022128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102858</td>\n",
       "      <td>Just saw two friends vape, Do I snitch? So I l...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>Just saw two friends vape, Do I snitch? So I l...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.726340</td>\n",
       "      <td>0.389685</td>\n",
       "      <td>0.347877</td>\n",
       "      <td>0.707561</td>\n",
       "      <td>0.022913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188810</td>\n",
       "      <td>Hey lads! Can I get some help from y'all? So.....</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>Hey lads! Can I get some help from y'all? So.....</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.753932</td>\n",
       "      <td>0.489636</td>\n",
       "      <td>0.230587</td>\n",
       "      <td>0.776817</td>\n",
       "      <td>0.031480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88124</td>\n",
       "      <td>Why do parents always bring up their “stories”...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>Why do parents always bring up their \"stories\"...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.802898</td>\n",
       "      <td>0.768912</td>\n",
       "      <td>0.275205</td>\n",
       "      <td>0.437977</td>\n",
       "      <td>0.032501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>272846</td>\n",
       "      <td>ContemplationI have everything I need to do It...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>Contemplation. I have everything I need to do ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.747140</td>\n",
       "      <td>0.331684</td>\n",
       "      <td>0.412662</td>\n",
       "      <td>0.536295</td>\n",
       "      <td>0.939585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>49620</td>\n",
       "      <td>I don't want to get better, I want to get wors...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>I don't want to get better, I want to get wors...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.437285</td>\n",
       "      <td>0.340145</td>\n",
       "      <td>0.125675</td>\n",
       "      <td>0.653872</td>\n",
       "      <td>0.946523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>291094</td>\n",
       "      <td>My only friend died and I want to go with him....</td>\n",
       "      <td>suicide</td>\n",
       "      <td>My only friend died and I want to go with him....</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.395193</td>\n",
       "      <td>0.474021</td>\n",
       "      <td>0.074487</td>\n",
       "      <td>0.620530</td>\n",
       "      <td>0.958698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>138491</td>\n",
       "      <td>I fantasize about dying all the timeI think th...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>I fantasize about dying all the time. I think ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.604205</td>\n",
       "      <td>0.529648</td>\n",
       "      <td>0.226070</td>\n",
       "      <td>0.512029</td>\n",
       "      <td>0.966551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>193733</td>\n",
       "      <td>Help me.I just want someone there for me. I'm ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>Help me.I just want someone there for me. I'm ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.761854</td>\n",
       "      <td>0.346296</td>\n",
       "      <td>0.210556</td>\n",
       "      <td>0.428369</td>\n",
       "      <td>1.020516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Column1                                               text    ori_class  \\\n",
       "0       79033  Here are some jokes that don't have a punchlin...  non-suicide   \n",
       "1      257115  I'll begin to blog on here before I end it on ...      suicide   \n",
       "2      102858  Just saw two friends vape, Do I snitch? So I l...  non-suicide   \n",
       "3      188810  Hey lads! Can I get some help from y'all? So.....  non-suicide   \n",
       "4       88124  Why do parents always bring up their “stories”...  non-suicide   \n",
       "...       ...                                                ...          ...   \n",
       "1995   272846  ContemplationI have everything I need to do It...      suicide   \n",
       "1996    49620  I don't want to get better, I want to get wors...      suicide   \n",
       "1997   291094  My only friend died and I want to go with him....      suicide   \n",
       "1998   138491  I fantasize about dying all the timeI think th...      suicide   \n",
       "1999   193733  Help me.I just want someone there for me. I'm ...      suicide   \n",
       "\n",
       "                                           cleaned_text   true_class  \\\n",
       "0     Here are some jokes that don't have a punchlin...  non-suicide   \n",
       "1     I'll begin to blog on here before I end it on ...      suicide   \n",
       "2     Just saw two friends vape, Do I snitch? So I l...  non-suicide   \n",
       "3     Hey lads! Can I get some help from y'all? So.....  non-suicide   \n",
       "4     Why do parents always bring up their \"stories\"...  non-suicide   \n",
       "...                                                 ...          ...   \n",
       "1995  Contemplation. I have everything I need to do ...      suicide   \n",
       "1996  I don't want to get better, I want to get wors...      suicide   \n",
       "1997  My only friend died and I want to go with him....      suicide   \n",
       "1998  I fantasize about dying all the time. I think ...      suicide   \n",
       "1999  Help me.I just want someone there for me. I'm ...      suicide   \n",
       "\n",
       "      openness  conscientiousness  extraversion  agreeableness  neuroticism  \n",
       "0     0.862079           0.632528      0.299561       0.650520     0.004304  \n",
       "1     0.561344           0.527004      0.222416       0.461801     0.022128  \n",
       "2     0.726340           0.389685      0.347877       0.707561     0.022913  \n",
       "3     0.753932           0.489636      0.230587       0.776817     0.031480  \n",
       "4     0.802898           0.768912      0.275205       0.437977     0.032501  \n",
       "...        ...                ...           ...            ...          ...  \n",
       "1995  0.747140           0.331684      0.412662       0.536295     0.939585  \n",
       "1996  0.437285           0.340145      0.125675       0.653872     0.946523  \n",
       "1997  0.395193           0.474021      0.074487       0.620530     0.958698  \n",
       "1998  0.604205           0.529648      0.226070       0.512029     0.966551  \n",
       "1999  0.761854           0.346296      0.210556       0.428369     1.020516  \n",
       "\n",
       "[2000 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/relabeled_2000_samples_2.csv', encoding='utf-8-sig')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57325315",
   "metadata": {},
   "source": [
    "#### 1\n",
    "- Original labels (2000 samples)\n",
    "- textual features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5595fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "texts = data['cleaned_text'].tolist()\n",
    "labels = data['ori_class'].map({'suicide': 1, 'non-suicide': 0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    token=True  \n",
    ")\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    texts.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels.values, dtype=torch.long)  # ensure integer labels\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,      \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base',config=config)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# use cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(4):\n",
    "\n",
    "    # train loop\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_probs = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # calculate probabilities for roc_auc_score\n",
    "        probs = torch.nn.functional.softmax(torch.from_numpy(logits), dim=1).numpy()\n",
    "        val_probs.extend(probs)\n",
    "\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        val_labels.extend(label_ids)\n",
    "        val_preds.extend(predictions)\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"  Val Acc:    {val_accuracy:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print('RoBERTa Results:')\n",
    "print(classification_report(val_labels, val_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c87653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-suicide', 'Suicide'], yticklabels=['Non-suicide', 'Suicide'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (RoBERTa) - Original labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897729d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_dir = \"../saved_models/roberta_raw\"\n",
    "\n",
    "#save misclassified samples\n",
    "val_indices = val_dataset.indices if hasattr(val_dataset, 'indices') else np.arange(len(val_dataset))\n",
    "\n",
    "val_texts = [texts[i] for i in val_indices]\n",
    "val_true_labels = [labels[i].item() for i in val_indices]  \n",
    "val_pred_labels = val_preds  \n",
    "\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'text': val_texts,\n",
    "    'true_label': val_true_labels,\n",
    "    'pred_label': val_pred_labels\n",
    "})\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified_df = val_df[val_df['true_label'] != val_df['pred_label']]\n",
    "\n",
    "# Optionally, map label numbers back to class names\n",
    "label_map = {0: 'non-suicide', 1: 'suicide'}\n",
    "misclassified_df['true_label_name'] = misclassified_df['true_label'].map(label_map)\n",
    "misclassified_df['pred_label_name'] = misclassified_df['pred_label'].map(label_map)\n",
    "\n",
    "print(f\"Number of misclassified samples: {len(misclassified_df)}\")\n",
    "print(misclassified_df.head(10))  # view first 10 misclassified samples\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "misclassified_df.to_csv(output_dir + '/misclassified_samples.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Model state dict and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7289e",
   "metadata": {},
   "source": [
    "### 2 \n",
    "- Original labels (2000 samples)\n",
    "- Textual features + Personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# === Load data including personality features ===\n",
    "texts = data['cleaned_text'].tolist()\n",
    "labels = data['ori_class'].map({'suicide': 1, 'non-suicide': 0})\n",
    "personality_features = data[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features = torch.tensor(personality_features, dtype=torch.float32)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    texts,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels.values, dtype=torch.long) \n",
    "\n",
    "# Dataset with text tokens + personality features\n",
    "class RoBertaWithPersonalityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, personality_feats, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.personality_feats = personality_feats\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx],\n",
    "                self.attention_masks[idx],\n",
    "                self.personality_feats[idx],\n",
    "                self.labels[idx])\n",
    "    \n",
    "dataset = RoBertaWithPersonalityDataset(input_ids, attention_masks, personality_features, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ===== Custom model combining RoBERTa+ Personality Features =====\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(\"roberta-base\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ===== Config =====\n",
    "config = AutoConfig.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,          \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "\n",
    "model = RoBertaWithPersonality.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# ===== Training Loop =====\n",
    "for epoch in range(4):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_personality, b_labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "        logits = model(b_input_ids, attention_mask=b_input_mask, personality_feats=b_personality)\n",
    "\n",
    "        loss = criterion(logits, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_labels, val_preds, val_probs = [], [], []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_personality, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, attention_mask=b_input_mask, personality_feats=b_personality)\n",
    "            loss = criterion(logits, b_labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Probabilities for ROC-AUC\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            val_probs.extend(probs)\n",
    "\n",
    "            predictions = np.argmax(logits.cpu().numpy(), axis=1)\n",
    "            val_labels.extend(label_ids)\n",
    "            val_preds.extend(predictions)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"  Val Acc:    {val_accuracy:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"RoBERTa Results:\")\n",
    "print(classification_report(val_labels, val_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-suicide', 'Suicide'], yticklabels=['Non-suicide', 'Suicide'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (RoBERTa) - Original labels + Personality (Benchmark)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc27a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "output_dir = \"../saved_models/roberta_raw_personality\"\n",
    "\n",
    "#save misclassified samples\n",
    "val_indices = val_dataset.indices if hasattr(val_dataset, 'indices') else np.arange(len(val_dataset))\n",
    "\n",
    "val_texts = [texts[i] for i in val_indices]\n",
    "val_true_labels = [labels[i].item() for i in val_indices]  \n",
    "val_pred_labels = val_preds  \n",
    "\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'text': val_texts,\n",
    "    'true_label': val_true_labels,\n",
    "    'pred_label': val_pred_labels\n",
    "})\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified_df = val_df[val_df['true_label'] != val_df['pred_label']]\n",
    "\n",
    "# Optionally, map label numbers back to class names\n",
    "label_map = {0: 'non-suicide', 1: 'suicide'}\n",
    "misclassified_df['true_label_name'] = misclassified_df['true_label'].map(label_map)\n",
    "misclassified_df['pred_label_name'] = misclassified_df['pred_label'].map(label_map)\n",
    "\n",
    "print(f\"Number of misclassified samples: {len(misclassified_df)}\")\n",
    "print(misclassified_df.head(10))  # view first 10 misclassified samples\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "misclassified_df.to_csv(output_dir + '/misclassified_samples.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Model state dict and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd5592",
   "metadata": {},
   "source": [
    "#### 3\n",
    "- relabeled 2000 samples\n",
    "- textual features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394efb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "\n",
    "texts = data['cleaned_text'].tolist()\n",
    "labels = data['true_class'].map({'suicide': 1, 'non-suicide': 0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    texts,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels.values, dtype=torch.long)  # ensure integer labels\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,      \n",
    "    attention_probs_dropout_prob=0.3  \n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# use cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(4):\n",
    "\n",
    "    # train loop\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # validation loop\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_probs = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # calculate probabilities for roc_auc_score\n",
    "        probs = torch.nn.functional.softmax(torch.from_numpy(logits), dim=1).numpy()\n",
    "        val_probs.extend(probs)\n",
    "\n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        val_labels.extend(label_ids)\n",
    "        val_preds.extend(predictions)\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "    print(f\"  Val Acc:    {val_accuracy:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print('RoBERTa Results:')\n",
    "print(classification_report(val_labels, val_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae95356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-suicide', 'Suicide'], yticklabels=['Non-suicide', 'Suicide'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (RoBERTa) - relabeled')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save misclassified samples\n",
    "val_indices = val_dataset.indices if hasattr(val_dataset, 'indices') else np.arange(len(val_dataset))\n",
    "\n",
    "val_texts = [texts[i] for i in val_indices]\n",
    "val_true_labels = [labels[i].item() for i in val_indices]  \n",
    "val_pred_labels = val_preds  \n",
    "\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'text': val_texts,\n",
    "    'true_label': val_true_labels,\n",
    "    'pred_label': val_pred_labels\n",
    "})\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified_df = val_df[val_df['true_label'] != val_df['pred_label']]\n",
    "\n",
    "# Optionally, map label numbers back to class names\n",
    "label_map = {0: 'non-suicide', 1: 'suicide'}\n",
    "misclassified_df['true_label_name'] = misclassified_df['true_label'].map(label_map)\n",
    "misclassified_df['pred_label_name'] = misclassified_df['pred_label'].map(label_map)\n",
    "\n",
    "print(f\"Number of misclassified samples: {len(misclassified_df)}\")\n",
    "print(misclassified_df.head(10))  \n",
    "\n",
    "# Save the model\n",
    "output_dir = \"../saved_models/roberta_relabeled\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "misclassified_df.to_csv(output_dir + '/misclassified_samples.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Model state dict and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f7a2a",
   "metadata": {},
   "source": [
    "#### 4\n",
    "- relabeled 2000 samples\n",
    "- textual features + personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a530e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "# === Load data including personality features ===\n",
    "texts = data['cleaned_text'].tolist()\n",
    "labels = data['true_class'].map({'suicide': 1, 'non-suicide': 0})\n",
    "personality_features = data[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features = torch.tensor(personality_features, dtype=torch.float32)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "encoded_data = tokenizer.batch_encode_plus(\n",
    "    texts,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = torch.tensor(labels.values, dtype=torch.long) \n",
    "\n",
    "# Dataset with text tokens + personality features\n",
    "class RoBertaWithPersonalityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, personality_feats, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.personality_feats = personality_feats\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx],\n",
    "                self.attention_masks[idx],\n",
    "                self.personality_feats[idx],\n",
    "                self.labels[idx])\n",
    "    \n",
    "dataset = RoBertaWithPersonalityDataset(input_ids, attention_masks, personality_features, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ===== Custom model combining RoBERTa + Personality Features =====\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(\"roberta-base\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ===== Config =====\n",
    "config = AutoConfig.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    attention_probs_dropout_prob=0.3\n",
    ")\n",
    "\n",
    "model = RoBertaWithPersonality.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# ===== Training Loop =====\n",
    "for epoch in range(4):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_personality, b_labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "        logits = model(b_input_ids, attention_mask=b_input_mask, personality_feats=b_personality)\n",
    "\n",
    "        loss = criterion(logits, b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_labels, val_preds, val_probs = [], [], []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        b_input_ids, b_input_mask, b_personality, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, attention_mask=b_input_mask, personality_feats=b_personality)\n",
    "            loss = criterion(logits, b_labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Probabilities for ROC-AUC\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "            val_probs.extend(probs)\n",
    "\n",
    "            predictions = np.argmax(logits.cpu().numpy(), axis=1)\n",
    "            val_labels.extend(label_ids)\n",
    "            val_preds.extend(predictions)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\" Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\" Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\" Val Acc: {val_accuracy:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"RoBERTa Results:\")\n",
    "print(classification_report(val_labels, val_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-suicide', 'Suicide'], yticklabels=['Non-suicide', 'Suicide'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix (RoBERTa) - relabeled + personality')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save misclassified samples\n",
    "val_indices = val_dataset.indices if hasattr(val_dataset, 'indices') else np.arange(len(val_dataset))\n",
    "\n",
    "val_texts = [texts[i] for i in val_indices]\n",
    "val_true_labels = [labels[i].item() for i in val_indices]  \n",
    "val_pred_labels = val_preds  \n",
    "\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'text': val_texts,\n",
    "    'true_label': val_true_labels,\n",
    "    'pred_label': val_pred_labels\n",
    "})\n",
    "\n",
    "# Filter misclassified samples\n",
    "misclassified_df = val_df[val_df['true_label'] != val_df['pred_label']]\n",
    "\n",
    "# Optionally, map label numbers back to class names\n",
    "label_map = {0: 'non-suicide', 1: 'suicide'}\n",
    "misclassified_df['true_label_name'] = misclassified_df['true_label'].map(label_map)\n",
    "misclassified_df['pred_label_name'] = misclassified_df['pred_label'].map(label_map)\n",
    "\n",
    "print(f\"Number of misclassified samples: {len(misclassified_df)}\")\n",
    "print(misclassified_df.head(10))  # view first 10 misclassified samples\n",
    "\n",
    "# Save the model\n",
    "output_dir = \"../saved_models/roberta_personality\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "misclassified_df.to_csv(output_dir + '/misclassified_samples.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Model state dict and tokenizer saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a94dab",
   "metadata": {},
   "source": [
    "### 5\n",
    "### Testing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45085af4",
   "metadata": {},
   "source": [
    "#### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b246cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[418  82]\n",
      " [ 11 489]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       0.97      0.84      0.90       500\n",
      "     Suicide       0.86      0.98      0.91       500\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.92      0.91      0.91      1000\n",
      "weighted avg       0.92      0.91      0.91      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa+ Original Labels\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ===== Load dataset =====\n",
    "df = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "\n",
    "# Map string labels to integers\n",
    "label_map = {\"non-suicide\": 0, \"suicide\": 1}\n",
    "true_labels_int = df['true_class'].map(label_map).values\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load RoBERTasuicide classifier =====\n",
    "model_dir = \"../saved_models/roberta_raw\"\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "bert_model.to(device).eval()\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# ===== Parameters =====\n",
    "batch_size = 16  # adjust based on GPU memory\n",
    "final_predictions = []\n",
    "\n",
    "# ===== Batching for RoBERTa inference =====\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "    \n",
    "    # RoBERTa tokenization\n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # RoBERTa predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    final_predictions.extend(batch_preds)\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Evaluation =====\n",
    "df['Predicted_Label'] = [\"non-suicide\" if p==0 else \"suicide\" for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels_int, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels_int, final_predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5c16d",
   "metadata": {},
   "source": [
    "#### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeb2142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[396 104]\n",
      " [  5 495]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       0.99      0.79      0.88       500\n",
      "     Suicide       0.83      0.99      0.90       500\n",
      "\n",
      "    accuracy                           0.89      1000\n",
      "   macro avg       0.91      0.89      0.89      1000\n",
      "weighted avg       0.91      0.89      0.89      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa + Original Labels + Personality Evaluation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ===== Load dataset =====\n",
    "df = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "true_labels = df['true_class'].map({'non-suicide':0, 'suicide':1}).values\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load personality features =====\n",
    "personality_features = df[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features = torch.tensor(personality_features, dtype=torch.float32).to(device)\n",
    "\n",
    "# ===== Tokenizer =====\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('../saved_models/roberta_raw_personality')\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(\"roberta-base\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# ===== Load pretrained RoBERTa + Personality model =====\n",
    "config = AutoConfig.from_pretrained('roberta-base', num_labels=2)\n",
    "bert_personality_model = RoBertaWithPersonality.from_pretrained('../saved_models/roberta_raw_personality', config=config)\n",
    "bert_personality_model.to(device).eval()\n",
    "\n",
    "# ===== Parameters =====\n",
    "batch_size = 16\n",
    "final_predictions = []\n",
    "\n",
    "# ===== Batching for RoBERTa + Personality inference =====\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "    batch_personality = personality_features[i:i+batch_size]\n",
    "\n",
    "    # RoBERTa tokenization\n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    # RoBERTa + Personality predictions\n",
    "    with torch.no_grad():\n",
    "        logits = bert_personality_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            personality_feats=batch_personality\n",
    "        )\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        final_predictions.extend(batch_preds)\n",
    "\n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Evaluation =====\n",
    "df['Predicted_Label'] = [\"non-suicide\" if p==0 else \"suicide\" for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, final_predictions, target_names=[\"Non-suicide\",\"Suicide\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49fbd7a",
   "metadata": {},
   "source": [
    "#### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9277fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[440  60]\n",
      " [ 18 482]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       0.96      0.88      0.92       500\n",
      "     Suicide       0.89      0.96      0.93       500\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.92      0.92      0.92      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa + Relabeled Data Evaluation\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ===== Load dataset =====\n",
    "df = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "\n",
    "# Map string labels to integers\n",
    "label_map = {\"non-suicide\": 0, \"suicide\": 1}\n",
    "true_labels_int = df['true_class'].map(label_map).values\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load RoBERTa suicide classifier =====\n",
    "model_dir = \"../saved_models/roberta_relabeled\"\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "bert_model.to(device).eval()\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# ===== Parameters =====\n",
    "batch_size = 16  # adjust based on GPU memory\n",
    "final_predictions = []\n",
    "\n",
    "# ===== Batching for RoBERTa inference =====\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "    \n",
    "    # RoBERTa tokenization\n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # RoBERTa predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    final_predictions.extend(batch_preds)\n",
    "    \n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Evaluation =====\n",
    "df['Predicted_Label'] = [\"non-suicide\" if p==0 else \"suicide\" for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels_int, final_predictions)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels_int, final_predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c774b",
   "metadata": {},
   "source": [
    "#### 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4e00f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[463  37]\n",
      " [ 26 474]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       0.95      0.93      0.94       500\n",
      "     Suicide       0.93      0.95      0.94       500\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa + Relabeled Data + Personality Evaluation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ===== Load dataset =====\n",
    "df = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "true_labels = df['true_class'].map({'non-suicide':0, 'suicide':1}).values\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load personality features =====\n",
    "personality_features = df[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features = torch.tensor(personality_features, dtype=torch.float32).to(device)\n",
    "\n",
    "# ===== Tokenizer =====\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('../saved_models/roberta_personality')\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(\"roberta-base\", config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "# ===== Load pretrained RoBERTa + Personality model =====\n",
    "config = AutoConfig.from_pretrained('roberta-base', num_labels=2)\n",
    "bert_personality_model = RoBertaWithPersonality.from_pretrained('../saved_models/roberta_personality', config=config)\n",
    "bert_personality_model.to(device).eval()\n",
    "\n",
    "# ===== Parameters =====\n",
    "batch_size = 16\n",
    "final_predictions = []\n",
    "\n",
    "# ===== Batching for RoBERTa + Personality inference =====\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "    batch_personality = personality_features[i:i+batch_size]\n",
    "\n",
    "    # RoBERTa tokenization\n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    # RoBERTa + Personality predictions\n",
    "    with torch.no_grad():\n",
    "        logits = bert_personality_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            personality_feats=batch_personality\n",
    "        )\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        final_predictions.extend(batch_preds)\n",
    "\n",
    "\n",
    "\n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Evaluation =====\n",
    "df['Predicted_Label'] = [\"non-suicide\" if p==0 else \"suicide\" for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, final_predictions, target_names=[\"Non-suicide\",\"Suicide\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91b426",
   "metadata": {},
   "source": [
    "#### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfff4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[458  42]\n",
      " [ 36 464]]\n",
      "\n",
      "False Positive Rate (FPR): 0.0840\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       0.93      0.92      0.92       500\n",
      "     Suicide       0.92      0.93      0.92       500\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.92      0.92      0.92      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa + Relabeled + NLI Evaluation \n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# ===== Load dataset =====\n",
    "df = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "true_labels = df['true_class'].values  \n",
    "\n",
    "label_map = {\"non-suicide\": 0, \"suicide\": 1}\n",
    "true_labels_int = df['true_class'].map(label_map).values\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load RoBERTa suicide classifier =====\n",
    "model_dir = \"../saved_models/roberta_relabeled\"\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "bert_model.to(device).eval()\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Negative hypotheses to detect supportive/awareness posts =====\n",
    "\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or information, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "# ===== Helper: NLI negative filtering =====\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(\n",
    "            post,\n",
    "            hypo,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    \n",
    "    # If the max negative entailment is high, the post is likely non-suicidal\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Parameters =====\n",
    "batch_size = 16  # adjust based on GPU memory\n",
    "final_predictions = []\n",
    "\n",
    "# ===== Batching for RoBERTa inference =====\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "    \n",
    "    # RoBERTa tokenization\n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # RoBERTa predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Apply NLI negative filtering\n",
    "    for text, pred in zip(batch_texts, batch_preds):\n",
    "        if pred == 1:  # initially predicted as suicide\n",
    "            is_negative = nli_negative_filter(text, threshold=0.65)\n",
    "            final_predictions.append(0 if is_negative else 1)\n",
    "        else:\n",
    "            final_predictions.append(pred)\n",
    "\n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Evaluation =====\n",
    "\n",
    "df['Predicted_Label'] = [\"non-suicide\" if p==0 else \"suicide\" for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels_int, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels_int, final_predictions, target_names=[\"Non-suicide\", \"Suicide\"], zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efcbec9",
   "metadata": {},
   "source": [
    "#### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1af7326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../saved_models/roberta_personality and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[464  36]\n",
      " [ 28 472]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-suicide       0.94      0.93      0.94       500\n",
      "     Suicide       0.93      0.94      0.94       500\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RoBERTa + Relabeled + Personality + NLI Evaluation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ===== Load dataset =====\n",
    "df = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "true_labels = df['true_class'].map({'non-suicide':0, 'suicide':1}).values\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load personality features =====\n",
    "personality_features = df[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features = torch.tensor(personality_features, dtype=torch.float32).to(device)\n",
    "\n",
    "model_dir = \"../saved_models/roberta_personality\"\n",
    "# ===== Tokenizer =====\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = RobertaModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# ===== Load pretrained RoBERTa + Personality model =====\n",
    "config = AutoConfig.from_pretrained(model_dir, num_labels=2)\n",
    "bert_personality_model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "bert_personality_model.to(device).eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Negative hypotheses for NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "# ===== Helper: NLI negative filtering =====\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(post, hypo, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  \n",
    "            entail_prob = probs[0].item()  # entailment probability\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Parameters =====\n",
    "batch_size = 16\n",
    "final_predictions = []\n",
    "\n",
    "# ===== Batching for RoBERTa + Personality inference =====\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "    batch_personality = personality_features[i:i+batch_size]\n",
    "\n",
    "    # RoBERTa tokenization\n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    # RoBERTa+ Personality predictions\n",
    "    with torch.no_grad():\n",
    "        logits = bert_personality_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            personality_feats=batch_personality\n",
    "        )\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Apply NLI negative filtering\n",
    "    for text, pred in zip(batch_texts, batch_preds):\n",
    "        if pred == 1:  # initially predicted as suicide\n",
    "            is_negative = nli_negative_filter(text, threshold=0.65)\n",
    "            final_predictions.append(0 if is_negative else 1)\n",
    "        else:\n",
    "            final_predictions.append(pred)\n",
    "\n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ===== Evaluation =====\n",
    "df['Predicted_Label'] = [\"non-suicide\" if p==0 else \"suicide\" for p in final_predictions]\n",
    "\n",
    "cm = confusion_matrix(true_labels, final_predictions)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, final_predictions, target_names=[\"Non-suicide\",\"Suicide\"], zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa + Relabeled + Personality + NLI Evaluation with Dual Optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "# ===== Load both datasets =====\n",
    "# Performance test dataset\n",
    "df_performance = pd.read_csv('../dataset/testing_1000_samples_2.csv')\n",
    "true_labels_perf = df_performance['true_class'].map({'non-suicide':0, 'suicide':1}).values\n",
    "\n",
    "# False positive test dataset\n",
    "df_fp = pd.read_csv('../dataset/false_positive_test_set_with_personality.csv')  \n",
    "true_labels_fp = [0] * len(df_fp)  # All should be non-suicide\n",
    "\n",
    "print(f\"Performance dataset size: {len(df_performance)}\")\n",
    "print(f\"FP test dataset size: {len(df_fp)}\")\n",
    "\n",
    "# ===== Device =====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===== Load personality features for both datasets =====\n",
    "personality_features_perf = df_performance[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features_perf = torch.tensor(personality_features_perf, dtype=torch.float32).to(device)\n",
    "\n",
    "personality_features_fp = df_fp[[\"extraversion\", \"agreeableness\", \"neuroticism\"]].values\n",
    "personality_features_fp = torch.tensor(personality_features_fp, dtype=torch.float32).to(device)\n",
    "\n",
    "model_dir = \"../saved_models/roberta_personality\"\n",
    "# ===== Tokenizer =====\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "class RoBertaWithPersonality(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, personality_feat_dim=3, num_labels=2):\n",
    "        super().__init__(config)\n",
    "        self.bert = AutoModel.from_pretrained(model_dir, config=config)\n",
    "        bert_hidden_size = config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size + personality_feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, personality_feats, labels=None):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        combined = torch.cat((cls_output, personality_feats), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# ===== Load pretrained BERT + Personality model =====\n",
    "config = AutoConfig.from_pretrained(model_dir, num_labels=2)\n",
    "bert_personality_model = RoBertaWithPersonality.from_pretrained(model_dir, config=config)\n",
    "bert_personality_model.to(device).eval()\n",
    "\n",
    "# ===== Load pretrained NLI model =====\n",
    "nli_model_name = \"tasksource/deberta-small-long-nli\"\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name).to(device).eval()\n",
    "nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "\n",
    "# ===== Negative hypotheses for NLI filtering =====\n",
    "negative_hypotheses = [\n",
    "    \"The author discusses suicide in general, awareness, or prevention, not personal suicidal thoughts.\",\n",
    "    \"The post shares support, resources, or hotlines, not the author's own suicidal intent.\",\n",
    "    \"The author reflects to inspire or thank others, not describing current suicidal thoughts.\"\n",
    "]\n",
    "\n",
    "# ===== Helper: NLI negative filtering =====\n",
    "def nli_negative_filter(post, threshold=0.65):\n",
    "    max_neg_entail_prob = 0\n",
    "    for hypo in negative_hypotheses:\n",
    "        inputs = nli_tokenizer(post, hypo, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = nli_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]  # [entail, neutral, contradict]\n",
    "            entail_prob = probs[0].item()  # entailment probability\n",
    "        max_neg_entail_prob = max(max_neg_entail_prob, entail_prob)\n",
    "    return max_neg_entail_prob >= threshold\n",
    "\n",
    "# ===== Function to evaluate dataset with specific threshold =====\n",
    "def evaluate_dataset(df, personality_features, true_labels, threshold, dataset_name=\"Dataset\"):\n",
    "    final_predictions = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_texts = df['cleaned_text'][i:i+batch_size].tolist()\n",
    "        batch_personality = personality_features[i:i+batch_size]\n",
    "\n",
    "        # BERT tokenization\n",
    "        encodings = bert_tokenizer(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "        # BERT + Personality predictions\n",
    "        with torch.no_grad():\n",
    "            logits = bert_personality_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                personality_feats=batch_personality\n",
    "            )\n",
    "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        # Apply NLI negative filtering with current threshold\n",
    "        for text, pred in zip(batch_texts, batch_preds):\n",
    "            if pred == 1:  # initially predicted as suicide\n",
    "                is_negative = nli_negative_filter(text, threshold=threshold)\n",
    "                final_predictions.append(0 if is_negative else 1)\n",
    "            else:\n",
    "                final_predictions.append(pred)\n",
    "\n",
    "        # Free GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(true_labels, final_predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(true_labels, final_predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, final_predictions, zero_division=0)\n",
    "    f1 = f1_score(true_labels, final_predictions, zero_division=0)\n",
    "    accuracy = np.mean(np.array(final_predictions) == true_labels)\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'dataset': dataset_name,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'fpr': fpr,\n",
    "        'fp': fp,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'predictions': final_predictions\n",
    "    }\n",
    "\n",
    "# ===== Multi-objective threshold optimization =====\n",
    "thresholds_to_test = [0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.70]\n",
    "results = []\n",
    "\n",
    "print(\"Testing thresholds for dual optimization (performance + FP reduction)...\")\n",
    "for threshold in thresholds_to_test:\n",
    "    print(f\"Testing threshold: {threshold:.2f}\")\n",
    "    \n",
    "    # Evaluate on performance dataset\n",
    "    result_perf = evaluate_dataset(df_performance, personality_features_perf, true_labels_perf, threshold, \"Performance\")\n",
    "    \n",
    "    # Evaluate on FP dataset\n",
    "    result_fp = evaluate_dataset(df_fp, personality_features_fp, true_labels_fp, threshold, \"FP_Test\")\n",
    "    \n",
    "    results.append(result_perf)\n",
    "    results.append(result_fp)\n",
    "    \n",
    "    print(f\"  Performance - F1: {result_perf['f1_score']:.4f}, FP: {result_perf['fp']}\")\n",
    "    print(f\"  FP Test - FPR: {result_fp['fpr']:.4f}, FP: {result_fp['fp']}\")\n",
    "\n",
    "# ===== Find optimal threshold using multi-objective optimization =====\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Separate results by dataset\n",
    "perf_results = results_df[results_df['dataset'] == 'Performance']\n",
    "fp_results = results_df[results_df['dataset'] == 'FP_Test']\n",
    "\n",
    "# Create combined scoring metric\n",
    "combined_scores = []\n",
    "for threshold in thresholds_to_test:\n",
    "    perf_data = perf_results[perf_results['threshold'] == threshold].iloc[0]\n",
    "    fp_data = fp_results[fp_results['threshold'] == threshold].iloc[0]\n",
    "    \n",
    "    # Weighted combination of F1 (performance) and 1-FPR (FP reduction)\n",
    "    # Adjust weights based on your priorities (0.7:0.3 favors performance, 0.5:0.5 balanced)\n",
    "    weight_performance = 0.8\n",
    "    weight_fp = 0.2\n",
    "    \n",
    "    # Normalize F1 score (0-1)\n",
    "    normalized_f1 = perf_data['f1_score']\n",
    "    \n",
    "    # Normalize FP reduction (1-FPR, higher is better)\n",
    "    normalized_fp_reduction = 1 - fp_data['fpr']\n",
    "    \n",
    "    # Combined score\n",
    "    combined_score = (weight_performance * normalized_f1) + (weight_fp * normalized_fp_reduction)\n",
    "    \n",
    "    combined_scores.append({\n",
    "        'threshold': threshold,\n",
    "        'combined_score': combined_score,\n",
    "        'f1_score': perf_data['f1_score'],\n",
    "        'fpr': fp_data['fpr'],\n",
    "        'fp_count': fp_data['fp'],\n",
    "        'performance_fp': perf_data['fp']\n",
    "    })\n",
    "\n",
    "combined_df = pd.DataFrame(combined_scores)\n",
    "\n",
    "df_valid = combined_df[combined_df['f1_score'] >= 0.93]\n",
    "\n",
    "# Select threshold with the highest combined score\n",
    "best_idx = df_valid['combined_score'].idxmax()\n",
    "best_threshold = df_valid.loc[best_idx, 'threshold']\n",
    "best_combined_score = combined_df.loc[best_idx, 'combined_score']\n",
    "best_f1 = df_valid.loc[best_idx, 'f1_score']\n",
    "best_fp = df_valid.loc[best_idx, 'fp_count']\n",
    "\n",
    "print(f\"Selected threshold: {best_threshold}\")\n",
    "print(f\"F1 Score: {best_f1}\")\n",
    "print(f\"False Positives: {best_fp}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-OBJECTIVE THRESHOLD OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(combined_df.round(4))\n",
    "print(f\"\\nBest threshold: {best_threshold:.2f}\")\n",
    "print(f\"Combined score: {best_combined_score:.4f}\")\n",
    "print(f\"Performance F1: {combined_df.loc[best_idx, 'f1_score']:.4f}\")\n",
    "print(f\"FP Test FPR: {combined_df.loc[best_idx, 'fpr']:.4f}\")\n",
    "print(f\"FP Count: {combined_df.loc[best_idx, 'fp_count']}\")\n",
    "\n",
    "# ===== Final evaluation with optimal threshold =====\n",
    "print(f\"\\nFinal evaluation with optimal threshold: {best_threshold:.2f}\")\n",
    "\n",
    "# Performance dataset\n",
    "final_perf = evaluate_dataset(df_performance, personality_features_perf, true_labels_perf, best_threshold, \"Performance_Final\")\n",
    "print(\"\\nPerformance Dataset Results:\")\n",
    "print(f\"F1 Score: {final_perf['f1_score']:.4f}\")\n",
    "print(f\"Precision: {final_perf['precision']:.4f}\")\n",
    "print(f\"Recall: {final_perf['recall']:.4f}\")\n",
    "print(f\"False Positives: {final_perf['fp']}\")\n",
    "\n",
    "# FP dataset\n",
    "final_fp = evaluate_dataset(df_fp, personality_features_fp, true_labels_fp, best_threshold, \"FP_Test_Final\")\n",
    "print(\"\\nFP Test Dataset Results:\")\n",
    "print(f\"False Positive Rate: {final_fp['fpr']:.4f}\")\n",
    "print(f\"False Positives: {final_fp['fp']}\")\n",
    "print(f\"True Negatives: {final_fp['tn']}\")\n",
    "\n",
    "# ===== Comparison with baseline (no NLI) =====\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPARISON WITH BASELINE (NO NLI FILTERING)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Baseline for performance dataset\n",
    "baseline_perf = evaluate_dataset(df_performance, personality_features_perf, true_labels_perf, 0.0, \"Baseline\")\n",
    "print(f\"Performance Baseline F1: {baseline_perf['f1_score']:.4f} → With NLI: {final_perf['f1_score']:.4f}\")\n",
    "print(f\"Performance Baseline FP: {baseline_perf['fp']} → With NLI: {final_perf['fp']}\")\n",
    "\n",
    "# Baseline for FP dataset (all should be 0 predictions)\n",
    "baseline_fp_predictions = []\n",
    "for i in range(0, len(df_fp), 16):\n",
    "    batch_personality = personality_features_fp[i:i+16]\n",
    "    batch_texts = df_fp['cleaned_text'][i:i+16].tolist()\n",
    "    \n",
    "    encodings = bert_tokenizer(\n",
    "        batch_texts,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = bert_personality_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            personality_feats=batch_personality\n",
    "        )\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        baseline_fp_predictions.extend(batch_preds.tolist())\n",
    "\n",
    "baseline_fp_count = sum(baseline_fp_predictions)\n",
    "print(f\"FP Test Baseline FP: {baseline_fp_count} → With NLI: {final_fp['fp']}\")\n",
    "print(f\"FP Reduction: {((baseline_fp_count - final_fp['fp']) / baseline_fp_count * 100):.1f}%\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(thresholds_to_test, perf_results['f1_score'], label='F1')\n",
    "plt.plot(thresholds_to_test, fp_results['fpr'], label='False Positives')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
